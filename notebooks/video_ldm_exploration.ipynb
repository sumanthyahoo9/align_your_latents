{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13d05ba2",
   "metadata": {},
   "source": [
    "#### Use this notebook to understand the Video LDM end-to-end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a608eb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "from einops import rearrange\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8649be2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoUnet(UNet):\n",
    "    \"\"\"\n",
    "    Video diffusion model\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        # UNet params (pass through to parent)\n",
    "        in_channels: int = 4,\n",
    "        out_channels: int = 4,\n",
    "        base_channels: int = 128,\n",
    "        channel_mult: tuple = (1, 2, 4, 4),\n",
    "        num_res_blocks: int = 2,\n",
    "        time_emb_dim: int = 1280,\n",
    "        attention_resolutions: tuple = (16, 8),\n",
    "        input_resolution: int = 64,\n",
    "        # Video-specific params\n",
    "        temporal_layer_type: str = \"attention\",\n",
    "        temporal_attention_heads: int = 8,\n",
    "        add_temporal_at_resolutions: tuple = (32, 16),\n",
    "    ):\n",
    "        super().__init__(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            base_channels=base_channels,\n",
    "            channel_mult=channel_mult,\n",
    "            num_res_blocks=num_res_blocks,\n",
    "            time_emb_dim=time_emb_dim,\n",
    "            attention_resolutions=attention_resolutions,\n",
    "            input_resolution=input_resolution)\n",
    "        \n",
    "        # Save the video specific params\n",
    "        self.temporal_layer_type = temporal_layer_type\n",
    "        self.temporal_attention_heads = temporal_attention_heads\n",
    "        self.add_temporal_at_resolutions = add_temporal_at_resolutions\n",
    "        \n",
    "        # Calculate channels at each level\n",
    "        channels = [base_channels * mult for mult in channel_mult]\n",
    "        # Track which blocks need temporal layers\n",
    "        self.temporal_block_indices = []  # Maps block index → True/False\n",
    "        self.temporal_layers = nn.ModuleList()\n",
    "        self.alphas = nn.ParameterList()\n",
    "\n",
    "        # ========== ENCODER ==========\n",
    "        current_res = input_resolution\n",
    "        block_idx = 0\n",
    "        for level, out_ch in enumerate(channels):\n",
    "            print(f\"=== ENCODER Level {level}: out_ch={out_ch}, current_res={current_res} ===\")\n",
    "            for _ in range(num_res_blocks):\n",
    "                print(f\"  Block {block_idx}: channels={out_ch}, res={current_res}\")\n",
    "                # Check if this resolution needs a temporal layer\n",
    "                if current_res in add_temporal_at_resolutions:\n",
    "                    print(f\"Block {block_idx}: Creating temporal at res={current_res}, channels={out_ch}\")\n",
    "                    self.temporal_block_indices.append(block_idx)\n",
    "                    self.temporal_layers.append(\n",
    "                        TemporalLayer(\n",
    "                            channels=out_ch,\n",
    "                            layer_type=temporal_layer_type,\n",
    "                            num_heads=temporal_attention_heads\n",
    "                        )\n",
    "                    )\n",
    "                    self.alphas.append(nn.Parameter(torch.ones(1)))\n",
    "                block_idx += 1\n",
    "            # Downsampling reduces resolution\n",
    "            if level < len(channels)-1:\n",
    "                current_res //= 2\n",
    "\n",
    "        # ========== BOTTLENECK ==========\n",
    "        print(f\"=== BOTTLENECK: channels={channels[-1]}, current_res={current_res} ===\")\n",
    "        num_bottleneck = 2 # 2 bottleneck layers\n",
    "        for _ in range(num_bottleneck):\n",
    "            if current_res in add_temporal_at_resolutions:\n",
    "                self.temporal_block_indices.append(block_idx)\n",
    "                self.temporal_layers.append(\n",
    "                    TemporalLayer(\n",
    "                        channels=channels[-1],\n",
    "                        layer_type=temporal_layer_type,\n",
    "                        num_heads=temporal_attention_heads\n",
    "                    )\n",
    "                )\n",
    "                self.alphas.append(nn.Parameter(torch.ones(1)))\n",
    "            block_idx += 1\n",
    "\n",
    "        # ========== DECODER ==========\n",
    "        # Decoder processes in reverse order and upsamples AFTER each level\n",
    "        for level in reversed(range(len(channels))):\n",
    "            out_ch = channels[level]\n",
    "            print(f\"=== DECODER Level {level}: out_ch={out_ch}, current_res={current_res} ===\")\n",
    "            \n",
    "            for block_num in range(num_res_blocks):\n",
    "                print(f\"  Block {block_idx}: channels={out_ch}, res={current_res}\")\n",
    "                \n",
    "                if current_res in add_temporal_at_resolutions and level > 0:\n",
    "                    print(f\"Decoder block {block_idx}: Creating temporal at res={current_res}, channels={out_ch}\")\n",
    "                    self.temporal_block_indices.append(block_idx)\n",
    "                    self.temporal_layers.append(\n",
    "                        TemporalLayer(\n",
    "                            channels=out_ch,\n",
    "                            layer_type=temporal_layer_type,\n",
    "                            num_heads=temporal_attention_heads\n",
    "                        )\n",
    "                    )\n",
    "                    self.alphas.append(nn.Parameter(torch.ones(1)))\n",
    "                \n",
    "                block_idx += 1\n",
    "    \n",
    "            # Upsample AFTER processing all blocks at this level\n",
    "            # (UpBlock upsamples at the end, so next level will be at higher res)\n",
    "            if level > 0:\n",
    "                print(f\"  After level {level}: upsampling {current_res} -> {current_res*2}\")\n",
    "                current_res *= 2\n",
    "            \n",
    "            print(f\"=== END Level {level}: current_res={current_res} ===\\n\")\n",
    "\n",
    "        print(f\"Created {len(self.temporal_layers)} temporal layers at resolutions {add_temporal_at_resolutions}\")\n",
    "        print(f\"\\nFINAL temporal_block_indices: {self.temporal_block_indices}\")\n",
    "        print(f\"Total encoder blocks: {len(self.encoder_blocks)}\")\n",
    "        print(f\"Total bottleneck blocks: {len(self.bottleneck)}\")\n",
    "        print(f\"Total decoder blocks: {len(self.decoder_blocks)}\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass with temporal processing\n",
    "        \n",
    "        Args:\n",
    "            x: (B, C, T, H, W) - video latents\n",
    "            t: (B,) - timesteps\n",
    "        \n",
    "        Returns:\n",
    "            (B, C, T, H, W) - processed video\n",
    "        \"\"\"\n",
    "        \n",
    "        # ========== SECTION 1: PREPARE INPUT ==========\n",
    "        B, C, T, H, W = x.shape\n",
    "        \n",
    "        # Get time embedding and repeat for each frame\n",
    "        time_emb = self.time_embed(t)  # (B, time_emb_dim)\n",
    "        time_emb = time_emb.repeat_interleave(T, dim=0)  # (B*T, time_emb_dim)\n",
    "        \n",
    "        # Reshape to spatial format\n",
    "        x = rearrange(x, \"b c t h w -> (b t) c h w\")\n",
    "        \n",
    "        # ========== SECTION 2: INITIAL CONV ==========\n",
    "        x = self.conv_in(x)\n",
    "        \n",
    "        # ========== SECTION 3: ENCODER WITH TEMPORAL ==========\n",
    "        skips = []\n",
    "        temporal_layer_idx = 0\n",
    "        downsample_idx = 0\n",
    "        \n",
    "        for block_idx, block in enumerate(self.encoder_blocks):\n",
    "            print(f\"Encoder block {block_idx}: x.shape={x.shape}\")\n",
    "            # Spatial processing\n",
    "            z = block(x, time_emb)\n",
    "            print(f\"  After block: z.shape={z.shape}\")\n",
    "            \n",
    "            # Temporal processing if needed\n",
    "            if block_idx in self.temporal_block_indices:\n",
    "                temp_layer = self.temporal_layers[temporal_layer_idx]\n",
    "                alpha = self.alphas[temporal_layer_idx]\n",
    "                \n",
    "                # Reshape to video\n",
    "                z_video = rearrange(z, \"(b t) c h w -> b c t h w\", b=B, t=T)\n",
    "                \n",
    "                # Apply temporal layer\n",
    "                z_prime_video = temp_layer(z_video)\n",
    "                \n",
    "                # Reshape back\n",
    "                z_prime = rearrange(z_prime_video, \"b c t h w -> (b t) c h w\")\n",
    "                \n",
    "                # Alpha mixing\n",
    "                x = alpha * z + (1 - alpha) * z_prime\n",
    "                \n",
    "                temporal_layer_idx += 1\n",
    "            else:\n",
    "                x = z\n",
    "            \n",
    "            # Save skip\n",
    "            skips.append(x)\n",
    "            \n",
    "            # Downsample if needed\n",
    "            if (block_idx + 1) % self.num_res_blocks == 0 and downsample_idx < len(self.encoder_downsamples):\n",
    "                x = self.encoder_downsamples[downsample_idx](x)\n",
    "                downsample_idx += 1\n",
    "        \n",
    "        # ========== SECTION 4: BOTTLENECK WITH TEMPORAL ==========\n",
    "        for block_idx, block in enumerate(self.bottleneck):\n",
    "            # Spatial processing\n",
    "            z = block(x, time_emb)\n",
    "            # Calculate global block index\n",
    "            global_block_idx = len(self.encoder_blocks) + block_idx\n",
    "            print(f\"Bottleneck block {block_idx} (global={global_block_idx}): x.shape={x.shape}\")\n",
    "            \n",
    "            # Temporal processing if needed\n",
    "            if global_block_idx in self.temporal_block_indices:\n",
    "                print(f\"  → Using temporal layer {temporal_layer_idx}\")\n",
    "                temp_layer = self.temporal_layers[temporal_layer_idx]\n",
    "                alpha = self.alphas[temporal_layer_idx]\n",
    "                \n",
    "                # Reshape to video\n",
    "                z_video = rearrange(z, \"(b t) c h w -> b c t h w\", b=B, t=T)\n",
    "                \n",
    "                # Apply temporal layer\n",
    "                z_prime_video = temp_layer(z_video)\n",
    "                \n",
    "                # Reshape back\n",
    "                z_prime = rearrange(z_prime_video, \"b c t h w -> (b t) c h w\")\n",
    "                \n",
    "                # Alpha mixing\n",
    "                x = alpha * z + (1 - alpha) * z_prime\n",
    "                \n",
    "                temporal_layer_idx += 1\n",
    "            else:\n",
    "                x = z\n",
    "        \n",
    "        # ========== SECTION 5: DECODER WITH TEMPORAL ==========\n",
    "        for block_idx, block in enumerate(self.decoder_blocks):\n",
    "            # Get skip connection\n",
    "            skip = skips.pop()\n",
    "            \n",
    "            # Spatial processing (UpBlock takes 3 args)\n",
    "            z = block(x, skip, time_emb)\n",
    "            print(f\"  After block: z.shape={z.shape}\")\n",
    "            # Calculate global block index\n",
    "            global_block_idx = len(self.encoder_blocks) + len(self.bottleneck) + block_idx\n",
    "            print(f\"Decoder block {block_idx} (global={global_block_idx}): x.shape={x.shape}\")\n",
    "            \n",
    "            # Temporal processing if needed\n",
    "            if global_block_idx in self.temporal_block_indices:\n",
    "                print(f\"  → Using temporal layer {temporal_layer_idx}\")\n",
    "                temp_layer = self.temporal_layers[temporal_layer_idx]\n",
    "                alpha = self.alphas[temporal_layer_idx]\n",
    "                \n",
    "                # Reshape to video\n",
    "                z_video = rearrange(z, \"(b t) c h w -> b c t h w\", b=B, t=T)\n",
    "                \n",
    "                # Apply temporal layer\n",
    "                z_prime_video = temp_layer(z_video)\n",
    "                \n",
    "                # Reshape back\n",
    "                z_prime = rearrange(z_prime_video, \"b c t h w -> (b t) c h w\")\n",
    "                \n",
    "                # Alpha mixing\n",
    "                x = alpha * z + (1 - alpha) * z_prime\n",
    "                \n",
    "                temporal_layer_idx += 1\n",
    "            else:\n",
    "                x = z\n",
    "        \n",
    "        # ========== SECTION 6: OUTPUT ==========\n",
    "        x = self.conv_out(x)\n",
    "        \n",
    "        # Reshape back to video format\n",
    "        x = rearrange(x, \"(b t) c h w -> b c t h w\", b=B, t=T)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58ce089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Video LDM\n",
    "B, C, T, H, W = 2, 4, 8, 64, 64\n",
    "model = VideoUnet(\n",
    "    in_channels=4,\n",
    "    out_channels=4,\n",
    "    base_channels=64,\n",
    "    channel_mult=(1, 2),\n",
    "    num_res_blocks=1,\n",
    "    input_resolution=64,\n",
    "    attention_resolutions=(),\n",
    "    add_temporal_at_resolutions=(32,)\n",
    ")\n",
    "x = torch.randn(B, C, T, H, W)\n",
    "t = torch.randint(0, 1000, (B,))\n",
    "out = model(x, t)\n",
    "assert out.shape == (B, C, T, H, W)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm_video_env (3.11.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
