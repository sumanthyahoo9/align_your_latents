{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff121d10",
   "metadata": {},
   "source": [
    "#### Use this notebook to understand how we add positional embeddings to the frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aa7923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78610c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Convert a time step into embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int = 1280):\n",
    "        \"\"\"\n",
    "        Converts timestep t (integer) → embedding (dim,)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "        # MLP: dim → 4*dim → dim\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(dim * 4, dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        t: (B,) timestep indices [0, 1000]\n",
    "        Returns: (B, dim) embeddings\n",
    "        \"\"\"\n",
    "        pos_enc = self.sinusoidal_embedding(t) # Shape (B, dim)\n",
    "        print(f\"\\n The positional encodings are of shape \\n {pos_enc.shape}\")\n",
    "        return self.mlp(pos_enc)\n",
    "    \n",
    "    def sinusoidal_embedding(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        t: (B,) → (B, dim) using sin/cos\n",
    "    \n",
    "        Formula:\n",
    "        emb[i] = sin(t / 10000^(2i/dim)) if i even\n",
    "            = cos(t / 10000^(2i/dim)) if i odd\n",
    "        \"\"\"\n",
    "        device = t.device\n",
    "        half_dim = self.dim // 2\n",
    "        print(f\"\\n The original dimension is {self.dim} and \\n the half dimension is {half_dim}\")\n",
    "        \n",
    "        # Frequencies\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        print(f\"\\n The embeddings are of shape \\n {emb.shape}\")\n",
    "        # Outer product\n",
    "        emb = t[:, None] * emb[None, :]  # (B, half_dim)\n",
    "        print(f\"\\n After taking the outer product of timesteps {t.shape}, the embeddings are of shape \\n {emb.shape}\")\n",
    "        \n",
    "        # Sin and cos\n",
    "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)  # (B, dim)\n",
    "        print(f\"\\n After taking the sine and cosine of the embeddings, the shape of the embedding tensor is \\n {emb.shape}\")\n",
    "        \n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752781f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "dim = 1280\n",
    "\n",
    "time_emb = TimeEmbedding(dim)\n",
    "t = torch.randint(0, 1000, (batch_size,))\n",
    "\n",
    "emb = time_emb(t)\n",
    "\n",
    "assert emb.shape == (batch_size, dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm_video_env (3.11.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
